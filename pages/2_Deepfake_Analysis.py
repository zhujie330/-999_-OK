import os
import face_recognition
import numpy as np
import streamlit as st
import torch
from PIL import Image
from matplotlib import pyplot as plt
from torchvision import models, transforms
from torchcam.utils import overlay_mask
from torchcam.methods import SmoothGradCAMpp
from torchvision.transforms.v2.functional import to_pil_image
from modelscope import snapshot_download
from draw_gradient import compute_gradient, visualize_heatmap
from saliency.gradcam import GradCAM
import tempfile
from utils_model import get_model_dir
import base64
from io import BytesIO
import warnings
st.set_page_config(page_title="Deepfake Detection", page_icon="ğŸ”¬")
st.sidebar.header("ğŸ”¬Deepfake Detection")
st.write("# Demo for Deepfake AnalysisğŸ”¬")
st.write("âš ï¸ ç”±äº Git LFS æµé‡å·²è¾¾ä¸Šçº¿ï¼Œè‡ªåŠ¨è½¬ä» ModelScope è”ç½‘åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å")
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', message='.*use_column_width.*')
model_dir = get_model_dir()
model_file_path = os.path.join(model_dir, 'model1.pth')
if os.path.exists(model_file_path):
    st.write("âœ”ï¸ æ¨¡å‹å·²åŠ è½½")
    st.write("âœ”ï¸ æ¥ä¸‹æ¥æ‚¨å¯ä»¥é€‰æ‹©ä½¿ç”¨ç³»ç»Ÿä¸ºæ‚¨å‡†å¤‡çš„ä¸€äº›æµ‹è¯•å›¾ç‰‡ æˆ–è€… é€‰æ‹©æ‚¨æœ¬åœ°æƒ³è¦ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œåˆ†æ")
else:
    st.write("âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¨å€™é‡è¯•")

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

def preprocess(img):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    faces = face_recognition.face_locations(img)
    top, right, bottom, left = faces[0]
    face_img = img[top:bottom, left:right, :]
    face_img = torch.tensor(face_img / 255.0).permute(2, 0, 1)
    image_tensor = transform(face_img).unsqueeze(0)
    return image_tensor

def show_feature_map(layer_index, image_tensor):
    if layer_index == 4:
        model_new = torch.nn.Sequential(*(list(model.children())[:-2]))
    else:
        layer = None
        for name, module in model.named_children():
            if name == f"layer{layer_index+1}":
                layer = module
                break
        if layer is None:
            raise ValueError(f"Layer not found in the model.")
        model_new = torch.nn.Sequential(*(list(model.children())[:list(model.children()).index(layer)]))
    model_new = model_new.to(device)
    feature_map = model_new(image_tensor.to(device))
    fig, axs = plt.subplots(8, 8, figsize=(10, 10))
    st.text(f'feature map after layer{layer_index}')
    for i in range(64):
        row = i // 8
        col = i % 8
        axs[row, col].imshow(feature_map[0, i].cpu().detach().numpy(), cmap='viridis')
        axs[row, col].axis('off')
    st.pyplot(fig)

device = torch.device('cpu')
model = models.resnet50(pretrained=False)
model.fc = torch.nn.Linear(2048, 2)
states = torch.load(f"{model_dir}/model1.pth", map_location=device)
states = states['model']
states = {key[2:]: value for key, value in states.items()}
model.load_state_dict(states)
model = model.to(device)
model.eval()

map = st.sidebar.radio(
    label="Which would you like to observe?",
    options=("Feature Map", "Saliency Map", "Class Activation Map"), index=None
)
def image_to_base64(img: Image.Image) -> str:
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode("utf-8")

# æ˜¾ç¤ºé»˜è®¤æµ‹è¯•å›¾ç‰‡å’Œä¸Šä¼ å…¥å£
if 'show_default' not in st.session_state:
    st.session_state.show_default = False
if 'selected_img' not in st.session_state:
    st.session_state.selected_img = None
st.markdown("## ä½¿ç”¨ä¸ºæ‚¨å‡†å¤‡çš„å›¾ç‰‡è¿›è¡Œåˆ†æ")
if st.button("ğŸ“ ä½¿ç”¨é»˜è®¤æµ‹è¯•å›¾ç‰‡"):
    st.session_state.show_default = True

if st.session_state.get("show_default", False):
    test_image_folder = './test/image'
    test_image_files = os.listdir(test_image_folder)

    if test_image_files:
        st.markdown("""
        <style>
        .card {
            height: 300px;
            display: flex;
            flex-direction: column;
            justify-content: space-between;
            margin-bottom: 20px;
            position: relative;
        }
        .card-img {
            flex: 1;
            display: flex;
            align-items: center;
            overflow: hidden;
            border-radius: 8px;
        }
        .card-img img {
            width: 100%;
            height: auto;
            object-fit: contain;
        }
        .card-footer {
            margin-top: auto;
            padding: 10px 0;
        }
        </style>
        """, unsafe_allow_html=True)

        cols = st.columns(3)
        for idx, img_file in enumerate(test_image_files):
            img_path = os.path.join(test_image_folder, img_file)
            with cols[idx % 3]:
                try:
                    img = Image.open(img_path).convert('RGB')
                    st.markdown(
                        f'''
                        <div class="card">
                            <div class="card-img">
                                <img src="data:image/png;base64,{image_to_base64(img)}">
                            </div>
                            <div class="card-footer">
                        ''',
                        unsafe_allow_html=True
                    )

                    if st.button(f"é€‰æ‹© {img_file}", key=f"select_{idx}"):
                        st.session_state.selected_img = img_path

                    st.markdown('</div></div>', unsafe_allow_html=True)

                except Exception as e:
                    st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡ {img_file}: {e}")

# å±•ç¤ºè¢«é€‰æ‹©çš„å›¾ç‰‡
if st.session_state.selected_img:
    st.success(f"å·²é€‰æ‹©: {os.path.basename(st.session_state.selected_img)}")
    img = Image.open(st.session_state.selected_img).convert('RGB')
    st.image(img, caption='é€‰ä¸­çš„æµ‹è¯•å›¾ç‰‡', use_container_width=True)
    img_array = np.array(img)
    image_tensor = preprocess(img_array)


st.markdown("## ä¸Šä¼ æ‚¨æƒ³è¦åˆ†æçš„æœ¬åœ°å›¾ç‰‡")
uploaded_file = st.file_uploader(label="**é€‰æ‹©ä½ è¦åˆ†æçš„å›¾ç‰‡**", type=['jpg', 'png', 'jpeg'])

if uploaded_file is not None:
    img = Image.open(uploaded_file).convert("RGB")
    st.image(img, caption='ä¸Šä¼ çš„å›¾ç‰‡', use_column_width=True)
    img_array = np.array(img)
    image_tensor = preprocess(img_array)

if 'image_tensor' in locals():
    if image_tensor is not None:
        if map == "Feature Map":
            col1, col2, col3 = st.columns(3)
            with col1:
                show_feature_map(2, image_tensor)
            with col2:
                show_feature_map(3, image_tensor)
            with col3:
                show_feature_map(4, image_tensor)
    
        elif map == "Saliency Map":
            gradient = compute_gradient(image_tensor.to(device), model)
            heatmap, saliency = visualize_heatmap(gradient)
            input_image_np = image_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()
            superimposed_img = heatmap + input_image_np
            superimposed_img /= np.max(superimposed_img)
    
            col1, col2, col3 = st.columns(3)
            with col1:
                fig, ax = plt.subplots()
                ax.imshow(input_image_np)
                ax.set_title('image')
                st.pyplot(fig)
    
            with col2:
                fig, ax = plt.subplots()
                ax.imshow(saliency)
                ax.set_title("saliency map")
                st.pyplot(fig)
    
            with col3:
                fig, ax = plt.subplots()
                ax.imshow(superimposed_img)
                ax.set_title("saliency map with image")
                st.pyplot(fig)
    
        elif map == "Class Activation Map":
            with SmoothGradCAMpp(model) as cam_extractor:
                out = model(image_tensor.to(device))
                activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)
            result = overlay_mask(to_pil_image((image_tensor.squeeze(0)*255).to(torch.uint8)),
                                  to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)
            input_image_np = image_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()
    
            col1, col2, col3 = st.columns(3)
            with col1:
                fig, ax = plt.subplots()
                ax.imshow(input_image_np)
                ax.set_title("input image")
                st.pyplot(fig)
    
            with col2:
                fig, ax = plt.subplots()
                ax.imshow(activation_map[0].squeeze(0).detach().cpu().numpy())
                ax.set_title("class activation map")
                st.pyplot(fig)
    
            with col3:
                fig, ax = plt.subplots()
                ax.imshow(result)
                ax.set_title("class activation map on image")
                st.pyplot(fig)
else:
    st.info("ğŸ“¸ è¯·å…ˆä¸Šä¼ å›¾ç‰‡æˆ–é€‰æ‹©é»˜è®¤æµ‹è¯•å›¾ç‰‡")
