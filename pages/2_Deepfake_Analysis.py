import os
import face_recognition
import numpy as np
import streamlit as st
import torch
from PIL import Image
from matplotlib import pyplot as plt
from torchvision import models, transforms
from torchcam.utils import overlay_mask
from torchcam.methods import SmoothGradCAMpp
from torchvision.transforms.v2.functional import to_pil_image
from modelscope import snapshot_download
from draw_gradient import compute_gradient, visualize_heatmap
from saliency.gradcam import GradCAM
import tempfile
from utils_model import get_model_dir

st.set_page_config(page_title="Deepfake Detection", page_icon="ğŸ”¬")
st.sidebar.header("ğŸ”¬Deepfake Detection")
st.write("# Demo for Deepfake AnalysisğŸ”¬")
st.write("âš ï¸ ç”±äº Git LFS æµé‡å·²è¾¾ä¸Šçº¿ï¼Œè‡ªåŠ¨è½¬ä» ModelScope è”ç½‘åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å")

model_dir = get_model_dir()
model_file_path = os.path.join(model_dir, 'model1.pth')
if os.path.exists(model_file_path):
    st.write("âœ”ï¸ æ¨¡å‹å·²åŠ è½½")
else:
    st.write("âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¨å€™é‡è¯•")

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

def preprocess(img):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    faces = face_recognition.face_locations(img)
    top, right, bottom, left = faces[0]
    face_img = img[top:bottom, left:right, :]
    face_img = torch.tensor(face_img / 255.0).permute(2, 0, 1)
    image_tensor = transform(face_img).unsqueeze(0)
    return image_tensor

def show_feature_map(layer_index, image_tensor):
    if layer_index == 4:
        model_new = torch.nn.Sequential(*(list(model.children())[:-2]))
    else:
        layer = None
        for name, module in model.named_children():
            if name == f"layer{layer_index+1}":
                layer = module
                break
        if layer is None:
            raise ValueError(f"Layer not found in the model.")
        model_new = torch.nn.Sequential(*(list(model.children())[:list(model.children()).index(layer)]))
    model_new = model_new.to(device)
    feature_map = model_new(image_tensor.to(device))
    fig, axs = plt.subplots(8, 8, figsize=(10, 10))
    st.text(f'feature map after layer{layer_index}')
    for i in range(64):
        row = i // 8
        col = i % 8
        axs[row, col].imshow(feature_map[0, i].cpu().detach().numpy(), cmap='viridis')
        axs[row, col].axis('off')
    st.pyplot(fig)

device = torch.device('cpu')
model = models.resnet50(pretrained=False)
model.fc = torch.nn.Linear(2048, 2)
states = torch.load(f"{model_dir}/model1.pth", map_location=device)
states = states['model']
states = {key[2:]: value for key, value in states.items()}
model.load_state_dict(states)
model = model.to(device)
model.eval()

map = st.sidebar.radio(
    label="Which would you like to observe?",
    options=("Feature Map", "Saliency Map", "Class Activation Map"), index=None
)

# æ˜¾ç¤ºé»˜è®¤æµ‹è¯•å›¾ç‰‡å’Œä¸Šä¼ å…¥å£
if 'show_default' not in st.session_state:
    st.session_state.show_default = False
if 'selected_img' not in st.session_state:
    st.session_state.selected_img = None

st.markdown("## ä½¿ç”¨ç³»ç»Ÿæµ‹è¯•å›¾ç‰‡")
if st.button("ğŸ“ æ˜¾ç¤ºç³»ç»Ÿæµ‹è¯•å›¾ç‰‡"):
    st.session_state.show_default = True

img_array = None
image_tensor = None

if st.session_state.show_default:
    test_image_folder = './test/image'
    test_image_files = os.listdir(test_image_folder)

    if test_image_files:
        cols = st.columns(3)
        for idx, img_file in enumerate(test_image_files):
            img_path = os.path.join(test_image_folder, img_file)
            with cols[idx % 3]:
                try:
                    img = Image.open(img_path).convert("RGB")
                    st.image(img, caption=img_file, use_container_width=True)
                    if st.button(f"é€‰æ‹© {img_file}", key=f"select_{idx}"):
                        st.session_state.selected_img = img_path
                        st.success(f"å·²é€‰æ‹©ï¼š{img_file}")
                except Exception as e:
                    st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡: {e}")

if st.session_state.selected_img:
    img = Image.open(st.session_state.selected_img).convert("RGB")
    st.image(img, caption='é€‰ä¸­çš„æµ‹è¯•å›¾ç‰‡', use_column_width=True)
    img_array = np.array(img)
    image_tensor = preprocess(img_array)

st.markdown("## ä¸Šä¼ ä½ è‡ªå·±çš„å›¾ç‰‡è¿›è¡Œåˆ†æ")
uploaded_file = st.file_uploader(label="**é€‰æ‹©ä½ è¦åˆ†æçš„å›¾ç‰‡**", type=['jpg', 'png', 'jpeg'])

if uploaded_file is not None:
    img = Image.open(uploaded_file).convert("RGB")
    st.image(img, caption='ä¸Šä¼ çš„å›¾ç‰‡', use_column_width=True)
    img_array = np.array(img)
    image_tensor = preprocess(img_array)

if image_tensor is not None:
    if map == "Feature Map":
        col1, col2, col3 = st.columns(3)
        with col1:
            show_feature_map(2, image_tensor)
        with col2:
            show_feature_map(3, image_tensor)
        with col3:
            show_feature_map(4, image_tensor)

    elif map == "Saliency Map":
        gradient = compute_gradient(image_tensor.to(device), model)
        heatmap, saliency = visualize_heatmap(gradient)
        input_image_np = image_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()
        superimposed_img = heatmap + input_image_np
        superimposed_img /= np.max(superimposed_img)

        col1, col2, col3 = st.columns(3)
        with col1:
            fig, ax = plt.subplots()
            ax.imshow(input_image_np)
            ax.set_title('image')
            st.pyplot(fig)

        with col2:
            fig, ax = plt.subplots()
            ax.imshow(saliency)
            ax.set_title("saliency map")
            st.pyplot(fig)

        with col3:
            fig, ax = plt.subplots()
            ax.imshow(superimposed_img)
            ax.set_title("saliency map with image")
            st.pyplot(fig)

    elif map == "Class Activation Map":
        with SmoothGradCAMpp(model) as cam_extractor:
            out = model(image_tensor.to(device))
            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)
        result = overlay_mask(to_pil_image((image_tensor.squeeze(0)*255).to(torch.uint8)),
                              to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)
        input_image_np = image_tensor.squeeze().permute(1, 2, 0).detach().cpu().numpy()

        col1, col2, col3 = st.columns(3)
        with col1:
            fig, ax = plt.subplots()
            ax.imshow(input_image_np)
            ax.set_title("input image")
            st.pyplot(fig)

        with col2:
            fig, ax = plt.subplots()
            ax.imshow(activation_map[0].squeeze(0).detach().cpu().numpy())
            ax.set_title("class activation map")
            st.pyplot(fig)

        with col3:
            fig, ax = plt.subplots()
            ax.imshow(result)
            ax.set_title("class activation map on image")
            st.pyplot(fig)
