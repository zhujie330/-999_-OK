import os
import face_recognition
import streamlit as st
import torch
import psutil
from PIL import Image
import cv2
import numpy as np
import time
import torchvision.transforms as transforms
from matplotlib import pyplot as plt
from torch import nn
from torch.utils.data import Dataset
from torchvision import models
import tempfile
import time
import os
import logging
import torch.nn.functional as F
import requests
from io import BytesIO
from utils_model import get_model_dir 

logging.basicConfig(level=logging.DEBUG)
from modelscope import snapshot_download
import base64

print("å“ˆå“ˆå“ˆ")
st.set_page_config(page_title="Deepfake Detection", page_icon="ğŸ”")
st.sidebar.header("ğŸ”Deepfake Detection")
st.write("# Demo for Deepfake DetectionğŸ”")
st.write("âš ï¸ ç”±äº Git LFS æµé‡å·²è¾¾ä¸Šçº¿ï¼Œè‡ªåŠ¨è½¬ä» ModelScope è”ç½‘åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å")

device = torch.device('cpu')
model_dir = get_model_dir()
model_file_path = os.path.join(model_dir, 'model1.pth')

if os.path.exists(model_file_path):
    st.write("âœ”ï¸ æ¨¡å‹å·²åŠ è½½")
else:
     st.write("âš ï¸ æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¨å€™é‡è¯•")

   
    st.write("âœ”ï¸ æ¨¡å‹å·²åŠ è½½, æ¥ä¸‹æ¥ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨ç³»ç»Ÿä¸ºæ‚¨å‡†å¤‡çš„ä¸€äº›æµ‹è¯•å›¾ç‰‡ æˆ–è€… é€‰æ‹©ä½ æœ¬åœ°æƒ³è¦ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œæ£€æµ‹")

print(f"Using device: {device}")

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


def detect_faces(image):
    # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
    return image, faces


def preprocess(img):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    # image = cv2.imread(img)
    # image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    faces = face_recognition.face_locations(img)
    top, right, bottom, left = faces[0]
    face_img = img[top:bottom, left:right, :]
    face_img = torch.tensor(face_img / 255.0).permute(2, 0, 1)
    image_tensor = transform(face_img).unsqueeze(0)
    return image_tensor


def predict_img(image_tensor):
    print("å¼€å§‹é¢„æµ‹")
    try:
        st.session_state.model.eval()
        with torch.no_grad():
            print("å°†å›¾åƒå¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡")
            image_tensor = image_tensor.to(device)
            print(f"å›¾åƒå¼ é‡å¤§å°: {image_tensor.size()}, æ•°æ®ç±»å‹: {image_tensor.dtype}")
            print("æ¨¡å‹æ¨ç†å¼€å§‹")
            logit = st.session_state.model(image_tensor)
            print("æ¨¡å‹æ¨ç†ç»“æŸ")
            cls = torch.argmax(logit, dim=1).item()
            confidence = torch.softmax(logit, dim=1)[0][cls]
            prediction = "real" if cls == 0 else "fake"
            print("é¢„æµ‹æˆåŠŸ")
            return prediction, confidence.item()
    except Exception as e:
        raise RuntimeError(f"é¢„æµ‹å‡ºé”™: {e}")


class Model(nn.Module):
    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):
        super(Model, self).__init__()
        model = models.resnext50_32x4d(weights=None)

        self.model = nn.Sequential(*list(model.children())[:-2])
        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)
        self.relu = nn.LeakyReLU()
        self.dp = nn.Dropout(0.4)
        self.linear1 = nn.Linear(2048, num_classes)
        self.avgpool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        batch_size, seq_length, c, h, w = x.shape
        x = x.view(batch_size * seq_length, c, h, w)
        fmap = self.model(x)
        x = self.avgpool(fmap)
        x = x.view(batch_size, seq_length, 2048)
        x_lstm, _ = self.lstm(x, None)
        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))


im_size = 112
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
sm = nn.Softmax()
inv_normalize = transforms.Normalize(mean=-1 * np.divide(mean, std), std=np.divide([1, 1, 1], std))


def im_convert(tensor):
    """ Display a tensor as an image. """
    image = tensor.to("cpu").clone().detach()
    image = image.squeeze()
    image = inv_normalize(image)
    image = image.numpy()
    image = image.transpose(1, 2, 0)
    image = image.clip(0, 1)
    # cv2.imwrite('./2.png',image*255)
    return image


def frame_extract(video):
    vidObj = cv2.VideoCapture(video)
    success = 1
    while success:
        success, image = vidObj.read()
        if success:
            yield image


class validation_dataset():
    def __init__(self, video, sequence_length=100):
        self.video = video
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        self.count = sequence_length

    def get_dataset(self):
        frames = []
        word = st.empty()
        word.text('âŒ›extract frame...')
        bar = st.progress(0)

        for i, frame in enumerate(frame_extract(self.video)):
            # if(i % a == first_frame):
            bar.progress(i + 1)
            time.sleep(0.1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces = face_recognition.face_locations(frame)
            try:
                top, right, bottom, left = faces[0]
                frame = frame[top:bottom, left:right, :]
            except:
                pass
            frames.append(self.transform(frame))
            if (len(frames) == self.count):
                break
        # print("no of frames",len(frames))
        frames = torch.stack(frames)
        frames = frames[:self.count]
        return frames.unsqueeze(0)


def image_to_base64(img):
    buffered = BytesIO()
    img.save(buffered, format="PNG")
    return base64.b64encode(buffered.getvalue()).decode()


choice = st.sidebar.radio(label="What do you want to detect?", options=('Image'), index=0)

if choice == 'Image':
    # ä½¿ç”¨session_stateä¿å­˜é»˜è®¤å›¾ç‰‡æ˜¾ç¤ºçŠ¶æ€
    if 'show_default' not in st.session_state:
        st.session_state.show_default = False

    if st.button("ğŸ“ ä½¿ç”¨é»˜è®¤æµ‹è¯•å›¾ç‰‡"):
        st.session_state.show_default = True

    if st.session_state.show_default:
        test_image_folder = './test/image'
        test_image_files = os.listdir(test_image_folder)
        if test_image_files:
            # æ·»åŠ CSSæ ·å¼
            st.markdown("""
            <style>
            .card {
                height: 300px;
                display: flex;
                flex-direction: column;
                justify-content: space-between;
                margin-bottom: 20px;
                position: relative;
            }
            .card-img {
                flex: 1;
                display: flex;
                align-items: center;
                overflow: hidden;
                border-radius: 8px;
            }
            .card-img img {
                width: 100%;
                height: auto;
                object-fit: contain;
            }
            .card-footer {
                margin-top: auto;
                padding: 10px 0;
            }
            </style>
            """, unsafe_allow_html=True)

            # åˆ›å»º3åˆ—å¸ƒå±€
            cols = st.columns(3)

            for idx, img_file in enumerate(test_image_files):
                img_path = os.path.join(test_image_folder, img_file)
                with cols[idx % 3]:  # è‡ªåŠ¨å¾ªç¯ä½¿ç”¨3åˆ—
                    try:
                        img = Image.open(img_path).convert('RGB')

                        # åˆ›å»ºå¡ç‰‡å¼å¸ƒå±€
                        st.markdown(
                            f'''
                            <div class="card">
                                <div class="card-img">
                                    <img src="data:image/png;base64,{image_to_base64(img)}">
                                </div>
                                <div class="card-footer">
                            ''',
                            unsafe_allow_html=True
                        )

                        # åœ¨åº•éƒ¨æ·»åŠ æŒ‰é’®
                        if st.button(f"é€‰æ‹© {img_file}", key=f"select_{idx}"):
                            st.session_state.selected_img = img_path

                        st.markdown('</div></div>', unsafe_allow_html=True)

                    except Exception as e:
                        st.error(f"æ— æ³•åŠ è½½å›¾ç‰‡ {img_file}: {e}")

            if 'selected_img' in st.session_state and st.session_state.selected_img:
                st.success(f"å·²é€‰æ‹©: {os.path.basename(st.session_state.selected_img)}")

                if st.button('â€‹â€‹**â€‹â€‹start to detectâ€‹â€‹**â€‹â€‹', key="detect_default"):
                    try:
                        img = Image.open(st.session_state.selected_img).convert('RGB')
                        img_array = np.array(img)
                        image_tensor = preprocess(img_array)

                        if 'model_loaded' not in st.session_state:
                            model = models.resnet50(pretrained=False)
                            model.fc = torch.nn.Linear(2048, 2)
                            states = torch.load(f"{model_dir}/model1.pth",
                                                map_location=torch.device("cpu"))
                            states = states['model']
                            states = {key[2:]: value for key, value in states.items()}
                            model.load_state_dict(states)
                            model = model.to(device)
                            model.eval()
                            st.session_state.model = model
                            st.session_state.model_loaded = True

                        # æ‰§è¡Œé¢„æµ‹
                        prediction, confidence = predict_img(image_tensor)
                        st.info(f"ğŸ“‹the face in image is â€‹â€‹**â€‹â€‹{prediction}â€‹â€‹**â€‹â€‹")
                        st.info(f"ğŸ“‹the confidence is â€‹â€‹**â€‹â€‹{confidence:.2f}â€‹â€‹**â€‹â€‹")

                    except Exception as e:
                        st.error(f"æ£€æµ‹å‡ºé”™: {e}")

    st.markdown("""
    <style>
        div[data-testid="stFileUploader"] label p {
            font-size: 24px !important;
            font-weight: bold !important;
        }
    </style>
    """, unsafe_allow_html=True)
    uploaded_file = st.file_uploader(label="â€‹**â€‹é€‰æ‹©æœ¬åœ°æƒ³è¦æ£€æµ‹çš„çš„å›¾ç‰‡â€‹**â€‹", type=['jpg', 'png', 'jpeg'])

else:
    # æ·»åŠ ä¸€ä¸ªæŒ‰é’®ç”¨äºé€‰æ‹©é»˜è®¤æµ‹è¯•è§†é¢‘
    if st.button("ğŸ“ ä½¿ç”¨é»˜è®¤æµ‹è¯•è§†é¢‘"):
        test_video_folder = './test/video'
        test_video_files = os.listdir(test_video_folder)
        if test_video_files:
            # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæµ‹è¯•è§†é¢‘
            video_path = os.path.join(test_video_folder, test_video_files[0])
            uploaded_file = open(video_path, 'rb')
            # é¢„è§ˆé»˜è®¤è§†é¢‘çš„ç¬¬ä¸€å¸§
            cap = cv2.VideoCapture(video_path)
            ret, frame = cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                st.image(frame, caption='é»˜è®¤æµ‹è¯•è§†é¢‘é¦–å¸§')
            cap.release()
            uploaded_file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆä»¥ä¾¿åç»­å¤„ç†
    # ä¿ç•™åŸå§‹æ–‡ä»¶ä¸Šä¼ å™¨
    uploaded_file = st.file_uploader(label="â€‹**â€‹é€‰æ‹©è¦åˆ¤æ–­çš„è§†é¢‘â€‹**â€‹", type=['mp4', 'avi'])

# æ˜¾ç¤ºç»“æœ
if uploaded_file is not None:
    if choice == 'Image':
        # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
        img = Image.open(uploaded_file).convert('RGB')
        st.image(img, caption='uploaded image')
        if 'model_loaded' not in st.session_state:
            model = models.resnet50(pretrained=False)
            model.fc = torch.nn.Linear(2048, 2)
            states = torch.load(f"{model_dir}/model1.pth", map_location=torch.device("cpu"))
            states = states['model']
            states = {key[2:]: value for key, value in states.items()}
            model.load_state_dict(states)
            model = model.to(device)
            model.eval()
            st.session_state.model = model
            st.session_state.model_loaded = True

        img_array = np.array(img)
        image_tensor = preprocess(img_array)
        # æ£€æµ‹äººè„¸æŒ‰é’®
        if st.button('**start to detect**'):
            print("å“ˆå“ˆ1")
            try:
                print(f"Memory usage before prediction: {psutil.virtual_memory().percent}%")
                prediction, confidence = predict_img(image_tensor)
                print(f"Memory usage after prediction: {psutil.virtual_memory().percent}%")
            except Exception as e:
                print(f"Error during prediction: {e}")
                st.error(f"Error during prediction: {e}")
            else:
                st.info(f"ğŸ“‹the face in image is **{prediction}**")
                st.info(f"ğŸ“‹the confidence is **{confidence}**")






