import os
import face_recognition
import streamlit as st
import torch
import psutil
from PIL import Image
import cv2
import numpy as np
import time
import torchvision.transforms as transforms
from matplotlib import pyplot as plt
from torch import nn
from torch.utils.data import Dataset
from torchvision import models
import tempfile
import time
import os
import logging
import torch.nn.functional as F
logging.basicConfig(level=logging.DEBUG)

print("å“ˆå“ˆå“ˆ")
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•ï¼Œå³ pages æ–‡ä»¶å¤¹
current_dir = os.path.dirname(os.path.abspath(__file__))
print(current_dir)
# å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œè°ƒè¯•
device = torch.device('cpu')

print(f"Using device: {device}")
# åŠ è½½äººè„¸æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


# æ£€æµ‹äººè„¸çš„å‡½æ•°
def detect_faces(image):
    # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    # åœ¨ç°åº¦å›¾ä¸Šæ£€æµ‹äººè„¸
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    # åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹åˆ°çš„äººè„¸
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
    return image, faces


def preprocess(img):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    # image = cv2.imread(img)
    # image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    faces = face_recognition.face_locations(img)
    top, right, bottom, left = faces[0]
    face_img = img[top:bottom, left:right, :]
    face_img = torch.tensor(face_img / 255.0).permute(2, 0, 1)
    image_tensor = transform(face_img).unsqueeze(0)
    return image_tensor


# def predict_img(image_tensor):
#     logit = model(image_tensor.to(device))
#     cls = torch.argmax(logit, dim=1).item()
#     confidence = torch.softmax(logit, dim=1)[0][cls]
#     prediction = "real" if cls == 0 else "fake"
#     return prediction, confidence
def predict_img(image_tensor):
    print("å¼€å§‹é¢„æµ‹")
    try:
        model.eval()
        with torch.no_grad():
            print("å°†å›¾åƒå¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡")
            image_tensor = image_tensor.to(device)
            print(f"å›¾åƒå¼ é‡å¤§å°: {image_tensor.size()}, æ•°æ®ç±»å‹: {image_tensor.dtype}")
            print("æ¨¡å‹æ¨ç†å¼€å§‹")
            logit = model(image_tensor)
            print("æ¨¡å‹æ¨ç†ç»“æŸ")
            cls = torch.argmax(logit, dim=1).item()
            confidence = torch.softmax(logit, dim=1)[0][cls]
            prediction = "real" if cls == 0 else "fake"
            print("é¢„æµ‹æˆåŠŸ")
            return prediction, confidence.item()
    except Exception as e:
        raise RuntimeError(f"é¢„æµ‹å‡ºé”™: {e}")

class Model(nn.Module):
    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):
        super(Model, self).__init__()
        model = models.resnext50_32x4d(weights=None)

        self.model = nn.Sequential(*list(model.children())[:-2])
        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)
        self.relu = nn.LeakyReLU()
        self.dp = nn.Dropout(0.4)
        self.linear1 = nn.Linear(2048, num_classes)
        self.avgpool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        batch_size, seq_length, c, h, w = x.shape
        x = x.view(batch_size * seq_length, c, h, w)
        fmap = self.model(x)
        x = self.avgpool(fmap)
        x = x.view(batch_size, seq_length, 2048)
        x_lstm, _ = self.lstm(x, None)
        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))


im_size = 112
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
sm = nn.Softmax()
inv_normalize = transforms.Normalize(mean=-1 * np.divide(mean, std), std=np.divide([1, 1, 1], std))


def im_convert(tensor):
    """ Display a tensor as an image. """
    image = tensor.to("cpu").clone().detach()
    image = image.squeeze()
    image = inv_normalize(image)
    image = image.numpy()
    image = image.transpose(1, 2, 0)
    image = image.clip(0, 1)
    # cv2.imwrite('./2.png',image*255)
    return image


def frame_extract(video):
    vidObj = cv2.VideoCapture(video)
    success = 1
    while success:
        success, image = vidObj.read()
        if success:
            yield image


class validation_dataset():
    def __init__(self, video, sequence_length=100):
        self.video = video
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        self.count = sequence_length

    def get_dataset(self):
        frames = []
        word = st.empty()
        word.text('âŒ›extract frame...')
        bar = st.progress(0)

        for i, frame in enumerate(frame_extract(self.video)):
            # if(i % a == first_frame):
            bar.progress(i + 1)
            time.sleep(0.1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces = face_recognition.face_locations(frame)
            try:
                top, right, bottom, left = faces[0]
                frame = frame[top:bottom, left:right, :]
            except:
                pass
            frames.append(self.transform(frame))
            if (len(frames) == self.count):
                break
        # print("no of frames",len(frames))
        frames = torch.stack(frames)
        frames = frames[:self.count]
        return frames.unsqueeze(0)


import torch.nn.functional as F
import streamlit as st
import logging

# è®¾ç½®æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger()

def predict(model, img):
    try:
        st.write("ğŸŸ¢ è¿›å…¥ predict å‡½æ•°")
        logger.info("ğŸŸ¢ è¿›å…¥ predict å‡½æ•°")
        
        st.write("ğŸ“¥ è¾“å…¥ç±»å‹:", type(img))
        logger.info(f"ğŸ“¥ è¾“å…¥ç±»å‹: {type(img)}")

        if isinstance(img, torch.Tensor):
            st.write("ğŸ“ è¾“å…¥ shape:", img.shape)
            logger.info(f"ğŸ“ è¾“å…¥ shape: {img.shape}")
        else:
            st.warning("âš ï¸ è¾“å…¥ img ä¸æ˜¯ torch.Tensorï¼")
            logger.warning("âš ï¸ è¾“å…¥ img ä¸æ˜¯ torch.Tensorï¼")

        img = img.to(device)
        st.write("âœ… img.to(device) æˆåŠŸ")
        logger.info("âœ… img.to(device) æˆåŠŸ")

        # å‰å‘ä¼ æ’­
        st.write("ğŸš€ æ­£åœ¨æ‰§è¡Œ model(img)")
        logger.info("ğŸš€ æ­£åœ¨æ‰§è¡Œ model(img)")
        
        with st.spinner('ğŸ§  æ­£åœ¨è¿›è¡Œæ¨¡å‹æ¨ç†...è¯·ç¨å€™ï¼ˆå¯èƒ½éœ€è¦40ç§’ï¼‰'):
            start_time = time.time()
            output = model(img)
            end_time = time.time()
        st.success(f"âœ… æ¨¡å‹æ¨ç†å®Œæˆï¼Œç”¨æ—¶ {end_time - start_time:.2f} ç§’")
        st.write("âœ… å‰å‘ä¼ æ’­å®Œæˆï¼Œè¿”å›ç±»å‹:", type(output))
        logger.info(f"âœ… å‰å‘ä¼ æ’­å®Œæˆï¼Œè¿”å›ç±»å‹: {type(output)}")

        # é˜²æ­¢æ¨¡å‹åªè¿”å›ä¸€ä¸ªç»“æœæ—¶å‡ºé”™
        if isinstance(output, tuple):
            fmap, logits = output
            st.write("ğŸ“¦ fmap shape:", fmap.shape)
            st.write("ğŸ“¦ logits shape:", logits.shape)
            logger.info(f"ğŸ“¦ fmap shape: {fmap.shape}")
            logger.info(f"ğŸ“¦ logits shape: {logits.shape}")
        else:
            fmap = None
            logits = output
            st.warning("âš ï¸ æ¨¡å‹åªè¿”å›äº†ä¸€ä¸ªå€¼ï¼Œå‡è®¾æ˜¯ logits")
            logger.warning("âš ï¸ æ¨¡å‹åªè¿”å›äº†ä¸€ä¸ªå€¼ï¼Œå‡è®¾æ˜¯ logits")

        # æƒé‡è·å–
        try:
            weight_softmax = model.linear1.weight.detach().cpu().numpy()
            st.write("ğŸ¯ è·å– linear1 æƒé‡æˆåŠŸï¼Œshape:", weight_softmax.shape)
            logger.info(f"ğŸ¯ è·å– linear1 æƒé‡æˆåŠŸï¼Œshape: {weight_softmax.shape}")
        except Exception as e:
            st.warning(f"âš ï¸ è·å– linear1 æƒé‡å¤±è´¥: {e}")
            logger.warning(f"âš ï¸ è·å– linear1 æƒé‡å¤±è´¥: {e}")

        logits = F.softmax(logits, dim=1)
        st.write("âœ… Softmax è®¡ç®—å®Œæˆ")
        logger.info("âœ… Softmax è®¡ç®—å®Œæˆ")

        _, prediction = torch.max(logits, 1)
        st.write("ğŸ“Š é¢„æµ‹ç»“æœæ ‡ç­¾:", int(prediction.item()))
        logger.info(f"ğŸ“Š é¢„æµ‹ç»“æœæ ‡ç­¾: {int(prediction.item())}")

        confidence = logits[:, int(prediction.item())].item() * 100
        st.write("ğŸ“ˆ é¢„æµ‹ç½®ä¿¡åº¦:", confidence)
        logger.info(f"ğŸ“ˆ é¢„æµ‹ç½®ä¿¡åº¦: {confidence}")

        return int(prediction.item()), confidence

    except Exception as e:
        st.error(f"âŒ æ¨¡å‹æ¨ç†å‡ºé”™: {e}")
        logger.error(f"âŒ æ¨¡å‹æ¨ç†å‡ºé”™: {e}")
        import traceback
        st.text(traceback.format_exc())
        logger.error(f"å¼‚å¸¸è¯¦æƒ…:\n{traceback.format_exc()}")
        raise RuntimeError(f"æ¨¡å‹æ¨ç†å‡ºé”™: {e}")




st.set_page_config(page_title="Deepfake Detection", page_icon="ğŸ”")
st.sidebar.header("ğŸ”Deepfake Detection")

st.write("# Demo for Deepfake DetectionğŸ”")
#choice = st.sidebar.radio(label="What do you want to detect?", options=('Image', 'Video'), index=0)
choice = st.sidebar.radio(label="What do you want to detect?", options=('Image',), index=0)

# ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘
if choice == 'Image':
    uploaded_file = st.file_uploader(label="**choose the image you want to judge**",type=['jpg', 'png', 'jpeg'])
else:
    uploaded_file = st.file_uploader(label="**choose the video you want to judge**",type=['mp4', 'avi'])

# add_selectbox = st.sidebar.selectbox(
#     label="How would you like to be contacted?",
#     options=("Email", "Home phone", "Mobile phone"),
#     key="t1"
# )

# æ˜¾ç¤ºç»“æœ
if uploaded_file is not None:
    if choice == 'Image':
        # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
        img = Image.open(uploaded_file).convert('RGB')
        st.image(img, caption='uploaded image')
        # model
        model = models.resnet50(pretrained=False)
        model.fc = torch.nn.Linear(2048, 2)
        #print("è¿™é‡Œ1")
        device = torch.device('cpu')
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # states = torch.load(
        #     os.path.join("D:\\å…¶ä»–\\wehchatfile\\WeChat Files\\wxid_3hhhdkir3jfj22\\FileStorage\\File\\2024-07",
        #                  "CNNSpot.pth"))

        
        states = torch.load("./model1.pth", map_location=torch.device("cpu"))

        #print("è¿™é‡Œ2")
        states = states['model']
        states = {key[2:]: value for key, value in states.items()}
        model.load_state_dict(states)
        model = model.to(device)
        model.eval()

        img_array = np.array(img)
        image_tensor = preprocess(img_array)
        # æ£€æµ‹äººè„¸æŒ‰é’®
        if st.button('**start to detect**'):
            print("å“ˆå“ˆ1")
            try:
                print(f"Memory usage before prediction: {psutil.virtual_memory().percent}%")
                prediction, confidence = predict_img(image_tensor)
                print(f"Memory usage after prediction: {psutil.virtual_memory().percent}%")
            except Exception as e:
                print(f"Error during prediction: {e}")
                st.error(f"Error during prediction: {e}")
            else:
                print("å“ˆå“ˆ2")
                st.info(f"ğŸ“‹the face in image is **{prediction}**")
                print("å“ˆå“ˆ3")
                st.info(f"ğŸ“‹the confidence is **{confidence}**")
                print("å“ˆå“ˆ4")


    else:
        # è¯»å–ä¸Šä¼ çš„è§†é¢‘
        video_file = uploaded_file.name
        video_bytes = uploaded_file.read()
        st.write("è§†é¢‘1")
        st.video(video_bytes)
        st.write("è§†é¢‘2")
        # æ£€æµ‹äººè„¸æŒ‰é’®
        if st.button('**start to detect**'):
            t1 = time.time()
            st.write("é—®é¢˜åœ¨è¿™1")
            # å°†äºŒè¿›åˆ¶æ•°æ®å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                st.write("é—®é¢˜åœ¨è¿™2")
                temp_file.write(video_bytes)
                st.write("é—®é¢˜åœ¨è¿™3")
                temp_file_path = temp_file.name
                st.write("é—®é¢˜åœ¨è¿™4")
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ›å»º VideoCapture å¯¹è±¡
            cap = cv2.VideoCapture(temp_file_path)
            st.write("é—®é¢˜åœ¨è¿™5")
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            st.write("é—®é¢˜åœ¨è¿™6")
            video_dataset = validation_dataset(temp_file_path)
            st.write("é—®é¢˜åœ¨è¿™7")
            video_dataset = video_dataset.get_dataset()
            st.write("é—®é¢˜åœ¨è¿™8")
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            #os.unlink(temp_file_path)
            model = Model(2).to(device)
            st.write("é—®é¢˜åœ¨è¿™9")
            path_to_model = './df_model.pt'
            st.write("é—®é¢˜åœ¨è¿™10")
            model.load_state_dict(torch.load(path_to_model, device))
            st.write("é—®é¢˜åœ¨è¿™11")
            model.eval()
            st.write("é—®é¢˜åœ¨è¿™12")
            prediction, confidence = predict(model, video_dataset)
            st.write("é—®é¢˜åœ¨è¿™13")
            if prediction == 0:
                st.write("é—®é¢˜åœ¨è¿™14")
                prediction = "real"
            else:
                prediction = "fake"

            st.info(f"ğŸ“‹the face in video is **{prediction}**")
            st.info(f"ğŸ“‹the confidence is **{confidence}**")



            st.info(f"ğŸ“‹the face in video is **{prediction}**")
            st.info(f"ğŸ“‹the confidence is **{confidence}**")
            # for _ in range(frame_count):
            #     ret, frame = cap.read()
            #     if not ret:
            #         break
            #     result_frame, _ = detect_faces(frame)
            #
            #     st.image(result_frame, caption='result', use_column_width=True)
            # t2 = time.time()
            # st.write(f"æ£€æµ‹å®Œæˆï¼Œæ€»å…±ç”¨æ—¶ {t2 - t1} ç§’ã€‚")
