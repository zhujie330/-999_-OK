import os
import face_recognition
import streamlit as st
import torch
import psutil
from PIL import Image
import cv2
import numpy as np
import time
import torchvision.transforms as transforms
from matplotlib import pyplot as plt
from torch import nn
from torch.utils.data import Dataset
from torchvision import models
import tempfile
import time
import os
import logging

logging.basicConfig(level=logging.DEBUG)

print("å“ˆå“ˆå“ˆ")
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•ï¼Œå³ pages æ–‡ä»¶å¤¹
current_dir = os.path.dirname(os.path.abspath(__file__))
print(current_dir)
# å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œè°ƒè¯•
device = torch.device('cpu')

print(f"Using device: {device}")
# åŠ è½½äººè„¸æ£€æµ‹å™¨
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')


# æ£€æµ‹äººè„¸çš„å‡½æ•°
def detect_faces(image):
    # è½¬æ¢ä¸ºç°åº¦å›¾åƒ
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    # åœ¨ç°åº¦å›¾ä¸Šæ£€æµ‹äººè„¸
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))
    # åœ¨åŸå§‹å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹åˆ°çš„äººè„¸
    for (x, y, w, h) in faces:
        cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
    return image, faces


def preprocess(img):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    # image = cv2.imread(img)
    # image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    faces = face_recognition.face_locations(img)
    top, right, bottom, left = faces[0]
    face_img = img[top:bottom, left:right, :]
    face_img = torch.tensor(face_img / 255.0).permute(2, 0, 1)
    image_tensor = transform(face_img).unsqueeze(0)
    return image_tensor


# def predict_img(image_tensor):
#     logit = model(image_tensor.to(device))
#     cls = torch.argmax(logit, dim=1).item()
#     confidence = torch.softmax(logit, dim=1)[0][cls]
#     prediction = "real" if cls == 0 else "fake"
#     return prediction, confidence
def predict_img(image_tensor):
    print("å¼€å§‹é¢„æµ‹")
    try:
        model.eval()
        with torch.no_grad():
            print("å°†å›¾åƒå¼ é‡ç§»åŠ¨åˆ°è®¾å¤‡")
            image_tensor = image_tensor.to(device)
            print(f"å›¾åƒå¼ é‡å¤§å°: {image_tensor.size()}, æ•°æ®ç±»å‹: {image_tensor.dtype}")
            print("æ¨¡å‹æ¨ç†å¼€å§‹")
            logit = model(image_tensor)
            print("æ¨¡å‹æ¨ç†ç»“æŸ")
            cls = torch.argmax(logit, dim=1).item()
            confidence = torch.softmax(logit, dim=1)[0][cls]
            prediction = "real" if cls == 0 else "fake"
            print("é¢„æµ‹æˆåŠŸ")
            return prediction, confidence.item()
    except Exception as e:
        raise RuntimeError(f"é¢„æµ‹å‡ºé”™: {e}")

class Model(nn.Module):
    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=2048, bidirectional=False):
        super(Model, self).__init__()
        model = models.resnext50_32x4d(pretrained=True)
        self.model = nn.Sequential(*list(model.children())[:-2])
        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)
        self.relu = nn.LeakyReLU()
        self.dp = nn.Dropout(0.4)
        self.linear1 = nn.Linear(2048, num_classes)
        self.avgpool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        batch_size, seq_length, c, h, w = x.shape
        x = x.view(batch_size * seq_length, c, h, w)
        fmap = self.model(x)
        x = self.avgpool(fmap)
        x = x.view(batch_size, seq_length, 2048)
        x_lstm, _ = self.lstm(x, None)
        return fmap, self.dp(self.linear1(x_lstm[:, -1, :]))


im_size = 112
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
sm = nn.Softmax()
inv_normalize = transforms.Normalize(mean=-1 * np.divide(mean, std), std=np.divide([1, 1, 1], std))


def im_convert(tensor):
    """ Display a tensor as an image. """
    image = tensor.to("cpu").clone().detach()
    image = image.squeeze()
    image = inv_normalize(image)
    image = image.numpy()
    image = image.transpose(1, 2, 0)
    image = image.clip(0, 1)
    # cv2.imwrite('./2.png',image*255)
    return image


def frame_extract(video):
    vidObj = cv2.VideoCapture(video)
    success = 1
    while success:
        success, image = vidObj.read()
        if success:
            yield image


class validation_dataset():
    def __init__(self, video, sequence_length=100):
        self.video = video
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        self.count = sequence_length

    def get_dataset(self):
        frames = []
        word = st.empty()
        word.text('âŒ›extract frame...')
        bar = st.progress(0)

        for i, frame in enumerate(frame_extract(self.video)):
            # if(i % a == first_frame):
            bar.progress(i + 1)
            time.sleep(0.1)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces = face_recognition.face_locations(frame)
            try:
                top, right, bottom, left = faces[0]
                frame = frame[top:bottom, left:right, :]
            except:
                pass
            frames.append(self.transform(frame))
            if (len(frames) == self.count):
                break
        # print("no of frames",len(frames))
        frames = torch.stack(frames)
        frames = frames[:self.count]
        return frames.unsqueeze(0)


def predict(model, img):
    fmap, logits = model(img.to(device))
    weight_softmax = model.linear1.weight.detach().cpu().numpy()
    logits = sm(logits)
    _, prediction = torch.max(logits, 1)
    confidence = logits[:, int(prediction.item())].item() * 100
    # print('confidence of prediction:', logits[:, int(prediction.item())].item() * 100)
    # idx = np.argmax(logits.detach().cpu().numpy())
    # bz, nc, h, w = fmap.shape
    # out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h * w)).T, weight_softmax[idx, :].T)
    # predict = out.reshape(h, w)
    # predict = predict - np.min(predict)
    # predict_img = predict / np.max(predict)
    # predict_img = np.uint8(255 * predict_img)
    # out = cv2.resize(predict_img, (im_size, im_size))
    # heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)
    # img = im_convert(img[:, -1, :, :, :])
    # result = heatmap * 0.5 + img * 0.8 * 255
    # cv2.imwrite('heatmap.png', result)
    return int(prediction.item()), confidence


st.set_page_config(page_title="Deepfake Detection", page_icon="ğŸ”")
st.sidebar.header("ğŸ”Deepfake Detection")

st.write("# Demo for Deepfake DetectionğŸ”")
choice = st.sidebar.radio(label="What do you want to detect?", options=('Image', 'Video'), index=0)

# ä¸Šä¼ å›¾ç‰‡æˆ–è§†é¢‘
if choice == 'Image':
    uploaded_file = st.file_uploader(label="**choose the image you want to judge**",type=['jpg', 'png', 'jpeg'])
else:
    uploaded_file = st.file_uploader(label="**choose the video you want to judge**",type=['mp4', 'avi'])

# add_selectbox = st.sidebar.selectbox(
#     label="How would you like to be contacted?",
#     options=("Email", "Home phone", "Mobile phone"),
#     key="t1"
# )

# æ˜¾ç¤ºç»“æœ
if uploaded_file is not None:
    if choice == 'Image':
        # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡
        img = Image.open(uploaded_file).convert('RGB')
        st.image(img, caption='uploaded image')
        # model
        model = models.resnet50(pretrained=False)
        model.fc = torch.nn.Linear(2048, 2)
        #print("è¿™é‡Œ1")
        device = torch.device('cpu')
        current_dir = os.path.dirname(os.path.abspath(__file__))
        # states = torch.load(
        #     os.path.join("D:\\å…¶ä»–\\wehchatfile\\WeChat Files\\wxid_3hhhdkir3jfj22\\FileStorage\\File\\2024-07",
        #                  "CNNSpot.pth"))
        states = torch.load("./CNNSpot.pth")
        #print("è¿™é‡Œ2")
        states = states['model']
        states = {key[2:]: value for key, value in states.items()}
        model.load_state_dict(states)
        model = model.to(device)
        model.eval()

        img_array = np.array(img)
        image_tensor = preprocess(img_array)
        # æ£€æµ‹äººè„¸æŒ‰é’®
        if st.button('**start to detect**'):
            print("å“ˆå“ˆ1")
            try:
                print(f"Memory usage before prediction: {psutil.virtual_memory().percent}%")
                prediction, confidence = predict_img(image_tensor)
                print(f"Memory usage after prediction: {psutil.virtual_memory().percent}%")
            except Exception as e:
                print(f"Error during prediction: {e}")
                st.error(f"Error during prediction: {e}")
            else:
                print("å“ˆå“ˆ2")
                st.info(f"ğŸ“‹the face in image is **{prediction}**")
                print("å“ˆå“ˆ3")
                st.info(f"ğŸ“‹the confidence is **{confidence}**")
                print("å“ˆå“ˆ4")


    else:
        # è¯»å–ä¸Šä¼ çš„è§†é¢‘
        video_file = uploaded_file.name
        video_bytes = uploaded_file.read()
        print("è§†é¢‘1")
        st.video(video_bytes)
        print("è§†é¢‘2")
        # æ£€æµ‹äººè„¸æŒ‰é’®
        if st.button('**start to detect**'):
            t1 = time.time()
            # å°†äºŒè¿›åˆ¶æ•°æ®å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                temp_file.write(video_bytes)
                temp_file_path = temp_file.name
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ›å»º VideoCapture å¯¹è±¡
            cap = cv2.VideoCapture(temp_file_path)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            video_dataset = validation_dataset(temp_file_path)
            video_dataset = video_dataset.get_dataset()
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            #os.unlink(temp_file_path)
            model = Model(2).to(device)
            path_to_model = '..\df_model.pt'
            model.load_state_dict(torch.load(path_to_model, device))
            model.eval()
            prediction, confidence = predict(model, video_dataset)
            if prediction == 0:
                prediction = "real"
            else:
                prediction = "fake"

            st.info(f"ğŸ“‹the face in video is **{prediction}**")
            st.info(f"ğŸ“‹the confidence is **{confidence}**")
            # for _ in range(frame_count):
            #     ret, frame = cap.read()
            #     if not ret:
            #         break
            #     result_frame, _ = detect_faces(frame)
            #
            #     st.image(result_frame, caption='result', use_column_width=True)
            # t2 = time.time()
            # st.write(f"æ£€æµ‹å®Œæˆï¼Œæ€»å…±ç”¨æ—¶ {t2 - t1} ç§’ã€‚")
